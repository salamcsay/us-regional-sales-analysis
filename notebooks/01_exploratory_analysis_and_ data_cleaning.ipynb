{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff127f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS & DATA CLEANING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "''' SECTION 1: DATA LOADING AND INITIAL INSPECTION '''\n",
    "print(\"\\n1. DATA LOADING AND INITIAL INSPECTION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Load the raw dataset\n",
    "df = pd.read_csv('../data/raw/US_Regional_Sales_Data.csv')\n",
    "\n",
    "print(f\"üìä Dataset shape: {df.shape}\")\n",
    "print(f\"üìã Columns ({len(df.columns)}): {list(df.columns)}\")\n",
    "print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\nüìà DATASET OVERVIEW:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nüîç FIRST 5 ROWS:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nüìä BASIC STATISTICS:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb67107",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' SECTION 2: DATA QUALITY ASSESSMENT '''\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. COMPREHENSIVE DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüîç MISSING VALUES ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Enhanced missing values analysis\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percent,\n",
    "    'Data Type': df.dtypes\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "# Only show columns with missing values\n",
    "missing_cols = missing_df[missing_df['Missing Count'] > 0]\n",
    "if len(missing_cols) > 0:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(missing_cols)\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found in the dataset!\")\n",
    "\n",
    "print(\"\\nüîÑ DUPLICATE RECORDS ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicate_count} ({duplicate_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(\"Sample duplicate rows:\")\n",
    "    print(df[df.duplicated()].head())\n",
    "\n",
    "# Check for duplicate order numbers (business logic validation)\n",
    "duplicate_orders = df['OrderNumber'].duplicated().sum()\n",
    "print(f\"Duplicate OrderNumbers: {duplicate_orders}\")\n",
    "\n",
    "print(\"\\nüéØ DATA CONSISTENCY CHECKS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Business logic validation\n",
    "inconsistencies = []\n",
    "\n",
    "# Check for negative values where they shouldn't exist\n",
    "numeric_cols = ['Order Quantity', 'Unit Cost', 'Unit Price']\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        # Convert to numeric if necessary\n",
    "        if df[col].dtype == 'object':\n",
    "            # Remove currency symbols and commas, then convert\n",
    "            temp_numeric = pd.to_numeric(df[col].replace(r'[\\$,]', '', regex=True), errors='coerce')\n",
    "        else:\n",
    "            temp_numeric = df[col]\n",
    "        negative_count = (temp_numeric < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            inconsistencies.append(f\"Negative {col}: {negative_count} records\")\n",
    "\n",
    "# Check discount range (should be 0-1)\n",
    "if 'Discount Applied' in df.columns:\n",
    "    invalid_discount = ((df['Discount Applied'] < 0) | (df['Discount Applied'] > 1)).sum()\n",
    "    if invalid_discount > 0:\n",
    "        inconsistencies.append(f\"Invalid discount values: {invalid_discount} records\")\n",
    "\n",
    "# Check date logic (OrderDate <= ShipDate <= DeliveryDate)\n",
    "if all(col in df.columns for col in ['OrderDate', 'ShipDate', 'DeliveryDate']):\n",
    "    # Convert to datetime first for comparison\n",
    "    df['OrderDate'] = pd.to_datetime(df['OrderDate'], format='mixed', dayfirst=True, errors='coerce')\n",
    "    df['ShipDate'] = pd.to_datetime(df['ShipDate'], format='mixed', dayfirst=True, errors='coerce')\n",
    "    df['DeliveryDate'] = pd.to_datetime(df['DeliveryDate'], format='mixed', dayfirst=True, errors='coerce')\n",
    "    \n",
    "    date_logic_issues = ((df['OrderDate'] > df['ShipDate']) | \n",
    "                        (df['ShipDate'] > df['DeliveryDate'])).sum()\n",
    "    if date_logic_issues > 0:\n",
    "        inconsistencies.append(f\"Date logic violations: {date_logic_issues} records\")\n",
    "\n",
    "if inconsistencies:\n",
    "    print(\"‚ö†Ô∏è  Data inconsistencies found:\")\n",
    "    for issue in inconsistencies:\n",
    "        print(f\"   ‚Ä¢ {issue}\")\n",
    "else:\n",
    "    print(\"‚úÖ No major data inconsistencies found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' SECTION 3: STATISTICAL PROFILING '''\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. STATISTICAL PROFILING & DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä SALES CHANNEL DISTRIBUTION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "channel_stats = df.groupby('Sales Channel').agg({\n",
    "    'OrderNumber': 'count',\n",
    "    'Order Quantity': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "channel_stats['Percentage'] = (channel_stats['OrderNumber'] / len(df) * 100).round(2)\n",
    "channel_stats = channel_stats.sort_values('OrderNumber', ascending=False)\n",
    "print(channel_stats)\n",
    "\n",
    "print(\"\\nüí∞ FINANCIAL METRICS DISTRIBUTION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Convert currency columns to numeric first\n",
    "currency_columns = ['Unit Cost', 'Unit Price']\n",
    "for col in currency_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# Calculate financial metrics for profiling\n",
    "if all(col in df.columns for col in ['Unit Price', 'Order Quantity', 'Discount Applied']):\n",
    "    df['Revenue'] = df['Unit Price'] * df['Order Quantity'] * (1 - df['Discount Applied'])\n",
    "    \n",
    "    financial_stats = {\n",
    "        'Total Revenue': df['Revenue'].sum(),\n",
    "        'Average Order Value': df['Revenue'].mean(),\n",
    "        'Median Order Value': df['Revenue'].median(),\n",
    "        'Revenue Std Dev': df['Revenue'].std(),\n",
    "        'Min Order Value': df['Revenue'].min(),\n",
    "        'Max Order Value': df['Revenue'].max()\n",
    "    }\n",
    "    \n",
    "    print(\"Financial Summary:\")\n",
    "    for metric, value in financial_stats.items():\n",
    "        print(f\"   ‚Ä¢ {metric}: ${value:,.2f}\")\n",
    "\n",
    "print(\"\\nüìÖ DATE RANGE ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "date_cols = ['OrderDate', 'ShipDate', 'DeliveryDate']\n",
    "for col in date_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"   ‚Ä¢ Range: {df[col].min()} to {df[col].max()}\")\n",
    "        print(f\"   ‚Ä¢ Span: {(df[col].max() - df[col].min()).days} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b4c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' SECTION 4: OUTLIER DETECTION '''\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. OUTLIER DETECTION & ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüîç STATISTICAL OUTLIER DETECTION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def detect_outliers_iqr(series, column_name):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "    \n",
    "    return {\n",
    "        'count': len(outliers),\n",
    "        'percentage': len(outliers) / len(series) * 100,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound\n",
    "    }\n",
    "\n",
    "# Check for outliers in key numeric columns\n",
    "numeric_columns = ['Order Quantity', 'Unit Price', 'Unit Cost']\n",
    "if 'Revenue' in df.columns:\n",
    "    numeric_columns.append('Revenue')\n",
    "\n",
    "outlier_summary = {}\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        outlier_info = detect_outliers_iqr(df[col], col)\n",
    "        outlier_summary[col] = outlier_info\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"   ‚Ä¢ Outliers: {outlier_info['count']} ({outlier_info['percentage']:.2f}%)\")\n",
    "        print(f\"   ‚Ä¢ Valid range: ${outlier_info['lower_bound']:.2f} to ${outlier_info['upper_bound']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a555c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' SECTION 5: ENHANCED FEATURE ENGINEERING '''\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. ENHANCED FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_enhanced_features(df):\n",
    "    \"\"\"\n",
    "    Create comprehensive feature set for advanced analytics\n",
    "    \"\"\"\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    print(\"üîß Creating financial metrics...\")\n",
    "    \n",
    "    # Core financial calculations\n",
    "    df_enhanced['Total Sales'] = df_enhanced['Unit Price'] * df_enhanced['Order Quantity']\n",
    "    df_enhanced['Total Revenue'] = df_enhanced['Order Quantity'] * df_enhanced['Unit Price'] * (1 - df_enhanced['Discount Applied'])\n",
    "    df_enhanced['Total Cost'] = df_enhanced['Unit Cost'] * df_enhanced['Order Quantity']\n",
    "    df_enhanced['Total Profit'] = df_enhanced['Total Revenue'] - df_enhanced['Total Cost']\n",
    "    df_enhanced['Profit Margin'] = np.where(df_enhanced['Total Revenue'] > 0, \n",
    "                                          df_enhanced['Total Profit'] / df_enhanced['Total Revenue'] * 100, 0)\n",
    "    df_enhanced['Discount Amount'] = df_enhanced['Unit Price'] * df_enhanced['Discount Applied'] * df_enhanced['Order Quantity']\n",
    "    \n",
    "    print(\"üìÖ Creating date-based features...\")\n",
    "    \n",
    "    # Enhanced date features\n",
    "    for date_col, prefix in [('OrderDate', 'Order'), ('ShipDate', 'Ship'), ('DeliveryDate', 'Delivery')]:\n",
    "        if date_col in df_enhanced.columns:\n",
    "            df_enhanced[f'{prefix} Year'] = df_enhanced[date_col].dt.year\n",
    "            df_enhanced[f'{prefix} Month'] = df_enhanced[date_col].dt.month\n",
    "            df_enhanced[f'{prefix} Day'] = df_enhanced[date_col].dt.day\n",
    "            df_enhanced[f'{prefix} Quarter'] = df_enhanced[date_col].dt.quarter\n",
    "            df_enhanced[f'{prefix} DayOfWeek'] = df_enhanced[date_col].dt.dayofweek\n",
    "            df_enhanced[f'{prefix} WeekOfYear'] = df_enhanced[date_col].dt.isocalendar().week\n",
    "    \n",
    "    print(\"‚è±Ô∏è Creating operational metrics...\")\n",
    "    \n",
    "    # Operational efficiency metrics\n",
    "    if all(col in df_enhanced.columns for col in ['OrderDate', 'ShipDate', 'DeliveryDate']):\n",
    "        df_enhanced['Processing_Days'] = (df_enhanced['ShipDate'] - df_enhanced['OrderDate']).dt.days\n",
    "        df_enhanced['Shipping_Days'] = (df_enhanced['DeliveryDate'] - df_enhanced['ShipDate']).dt.days\n",
    "        df_enhanced['Total_Lead_Time'] = (df_enhanced['DeliveryDate'] - df_enhanced['OrderDate']).dt.days\n",
    "        \n",
    "        # Categorize delivery performance\n",
    "        df_enhanced['Delivery_Category'] = pd.cut(df_enhanced['Total_Lead_Time'], \n",
    "                                                bins=[0, 3, 7, 14, float('inf')],\n",
    "                                                labels=['Express', 'Standard', 'Slow', 'Very Slow'])\n",
    "    \n",
    "    print(\"üéØ Creating business intelligence features...\")\n",
    "    \n",
    "    # Customer value segmentation preparation\n",
    "    df_enhanced['Order_Value_Category'] = pd.cut(df_enhanced['Total Revenue'], \n",
    "                                               bins=5, labels=['Low', 'Below Avg', 'Average', 'Above Avg', 'High'])\n",
    "    \n",
    "    # Product performance indicators\n",
    "    df_enhanced['High_Margin_Product'] = df_enhanced['Profit Margin'] > df_enhanced['Profit Margin'].median()\n",
    "    df_enhanced['Discounted_Order'] = df_enhanced['Discount Applied'] > 0\n",
    "    df_enhanced['Large_Order'] = df_enhanced['Order Quantity'] > df_enhanced['Order Quantity'].quantile(0.75)\n",
    "    \n",
    "    # Seasonal indicators\n",
    "    df_enhanced['Is_Holiday_Season'] = df_enhanced['Order Month'].isin([11, 12, 1])  # Nov, Dec, Jan\n",
    "    df_enhanced['Is_Summer'] = df_enhanced['Order Month'].isin([6, 7, 8])  # Jun, Jul, Aug\n",
    "    \n",
    "    print(\"‚úÖ Feature engineering completed!\")\n",
    "    print(f\"   ‚Ä¢ Features added: {len(df_enhanced.columns) - len(df.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Total features: {len(df_enhanced.columns)}\")\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# Apply enhanced feature engineering\n",
    "df_enhanced = create_enhanced_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e173ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' SECTION 6: DATA VALIDATION & QUALITY REPORT '''\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. DATA VALIDATION & QUALITY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def generate_quality_report(df):\n",
    "    \"\"\"Generate comprehensive data quality report\"\"\"\n",
    "    \n",
    "    quality_metrics = {\n",
    "        'Total Records': len(df),\n",
    "        'Total Features': len(df.columns),\n",
    "        'Memory Usage (MB)': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'Duplicate Records': df.duplicated().sum(),\n",
    "        'Missing Values': df.isnull().sum().sum(),\n",
    "        'Data Types': df.dtypes.value_counts().to_dict()\n",
    "    }\n",
    "    \n",
    "    print(\"üìã DATA QUALITY SUMMARY\")\n",
    "    print(\"-\" * 30)\n",
    "    for metric, value in quality_metrics.items():\n",
    "        if metric != 'Data Types':\n",
    "            print(f\"   ‚Ä¢ {metric}: {value}\")\n",
    "    \n",
    "    print(\"\\nüìä DATA TYPE DISTRIBUTION\")\n",
    "    print(\"-\" * 30)\n",
    "    for dtype, count in quality_metrics['Data Types'].items():\n",
    "        print(f\"   ‚Ä¢ {dtype}: {count} columns\")\n",
    "    \n",
    "    # Feature categories\n",
    "    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    datetime_features = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nüî¢ FEATURE CATEGORIES\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"   ‚Ä¢ Numeric features: {len(numeric_features)}\")\n",
    "    print(f\"   ‚Ä¢ Categorical features: {len(categorical_features)}\")\n",
    "    print(f\"   ‚Ä¢ DateTime features: {len(datetime_features)}\")\n",
    "    \n",
    "    return quality_metrics\n",
    "\n",
    "quality_report = generate_quality_report(df_enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f68cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' SECTION 7: SAVE PROCESSED DATA '''\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. SAVING PROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save the fully cleaned and enhanced dataset\n",
    "output_path = '../data/processed/fully_cleaned_us_regional_sales_data.csv'\n",
    "df_enhanced.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Enhanced dataset saved successfully!\")\n",
    "print(f\"üìç Location: {output_path}\")\n",
    "print(f\"üìä Final shape: {df_enhanced.shape}\")\n",
    "\n",
    "# Save data quality report\n",
    "report_path = '../reports/data_quality_report.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"DATA QUALITY REPORT\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    f.write(f\"Generated: {datetime.now()}\\n\\n\")\n",
    "    \n",
    "    f.write(\"DATASET OVERVIEW:\\n\")\n",
    "    f.write(f\"- Records: {len(df_enhanced):,}\\n\")\n",
    "    f.write(f\"- Features: {len(df_enhanced.columns)}\\n\")\n",
    "    f.write(f\"- Memory: {df_enhanced.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\n\\n\")\n",
    "    \n",
    "    f.write(\"DATA QUALITY METRICS:\\n\")\n",
    "    for metric, value in quality_report.items():\n",
    "        if metric != 'Data Types':\n",
    "            f.write(f\"- {metric}: {value}\\n\")\n",
    "\n",
    "print(f\"üìã Data quality report saved: {report_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENHANCED EDA & DATA CLEANING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "üéØ SUMMARY:\n",
    "   ‚Ä¢ Processed {len(df_enhanced):,} records\n",
    "   ‚Ä¢ Created {len(df_enhanced.columns) - len(df.columns)} additional features\n",
    "   ‚Ä¢ Data quality validated and documented\n",
    "   ‚Ä¢ Ready for advanced analytics!\n",
    "\n",
    "üìÅ OUTPUT FILES:\n",
    "   ‚Ä¢ Enhanced dataset: fully_cleaned_us_regional_sales_data.csv\n",
    "   ‚Ä¢ Quality report: data_quality_report.txt\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
